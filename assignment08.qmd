---
title: "Data Science for Public Policy"
subtitle: "Assignment 08"
author: "Priscila Stisman / Asad Azhar"
execute:
  warning: false
format:
  html:
    embed-resources: true
---

### Packages

```{r}
library (tidyverse)
library(tidymodels)
library(lubridate)
library(themis)
library(yardstick)
library(vip)
library(kknn)
library(patchwork)
library(tidyclust)
library(purrr)
library(readr)
```

# Exercise 1

## Part a

```{r}
votes_103 <- read_csv("votes_time_series.csv") %>%
  mutate_all(~ifelse(is.na(.), 0, .)) %>%
  filter(session == 103)

```

## Part b

```{r}

#Creating the recipe
votes_pca_rec <- votes_103 %>%
  recipe() %>%
  step_pca(starts_with("v"), num_comp = 5) %>%
  prep()


```

## Part c

```{r}

#Analysing PCs
tidy(votes_pca_rec, number = 1, type = "variance") %>%
  filter(terms == "cumulative percent variance") %>%
  slice_min(value, n = 5)

#The first PC explains around 39% of the variance while all of 5 of them cumulatively explain 68% of the variance.

## Part d


# Analysing PCs
votes_pcs <- votes_pca_rec %>%
  bake(new_data = votes_103)

# Loading and cleaning states data
states <- read_csv("states_regions.csv") %>%
  janitor::clean_names() %>%
  rename(state_name = state, state= state_code)

# Appending states data with votes PC data
votes_pcs <- left_join(votes_pcs, states, by = "state")

# Scatterplot with color mapped to party
plot_party <- ggplot(data = votes_pcs, aes(x = PC1, y = PC2, color = party)) +
  geom_point() +
  labs(title = "Scatterplot of PC1 and PC2 (Color by Party)",
       x = "PC1", y = "PC2")

# Scatterplot with color mapped to region
plot_region <- ggplot(data = votes_pcs, aes(x = PC1, y = PC2, color = region)) +
  geom_point() +
  labs(title = "Scatterplot of PC1 and PC2 (Color by Region)",
       x = "PC1", y = "PC2")

# Arrange plots side by side
combined_plots <- plot_party + plot_region

combined_plots

```


# Exercise 2

## Part a

```{r}
# Set seed for reproducibility
set.seed(20220412)

kmeans_rec <- recipe(formula = ~ .,data = votes_103) %>%
  step_select(all_numeric())

votes_cv <- vfold_cv(votes_103, v = 5)

kmeans_spec <- k_means(
  num_clusters = tune()) %>%
set_engine("stats", nstart = 100)

kmeans_wflow <- workflow(
  preprocessor = kmeans_rec,
  spec = kmeans_spec)

clust_num_grid <- grid_regular(num_clusters(),levels = 10)

res <- tune_cluster(kmeans_wflow,
                    resamples = votes_cv,
                    grid = clust_num_grid,
                    control = control_grid(save_pred = TRUE, extract = identity),
                    metrics = cluster_metric_set(sse_within_total, silhouette_avg))


res_metrics <- res %>%
  collect_metrics()

```


## Part b

```{r}



graph_pca_cluster <- function(df, clust) {
  
  # Creating the recipe for PCA
  rec_pca <- recipe(formula = ~ .,data = df) %>%
  step_pca(all_numeric(), id = "pca")
  
  # Creating recipe for kmeans
  kmeans_rec <- recipe(formula = ~ .,data = df) %>%
  step_select(all_numeric())
  
  #Completing PCA
  votes_pca <- rec_pca %>%
    prep() %>%
    bake(new_data = df)
  
  # Creating spec for clusters
  kmeans_spec <- k_means(num_clusters = clust) %>%
    set_engine("stats",nstart = 100)
  
  #Creating workflow
  kmeans_wflow <- workflow(
    preprocessor = kmeans_rec,
    spec = kmeans_spec)
  
  #Fitting data
  kmeans<- kmeans_wflow %>%
    fit(data = df)
  
  #Graphing PCA1 and PCA2 with color coded by cluster assigned.
  graph <- bind_cols(
    select(votes_pca, PC1, PC2),
    cluster = votes_kmeans %>%
      extract_cluster_assignment() %>%
      pull(.cluster)
)
  ggplot(graph, aes(x = PC1, y = PC2, color = factor(cluster))) +
    geom_point() +
    labs(title = "Scatterplot of PC1 vs PC2 with Cluster Assignment",
         x = "PC1", y = "PC2", color = "Cluster") +
    theme_minimal()
}

graph_pca_cluster(df = votes_103, clust = 2)

```


# Exercise 03
## Part a
```{r}
library(tidytext)

executive_orders <- read_csv("executive-orders.csv") %>%
  filter(!is.na(text)) # Filter out rows with missing text

# Create bigrams using unnest_tokens
executive_orders_bigrams <- executive_orders %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

## Part b
```{r}
# Load required libraries
install.packages("ggraph")
library(ggraph)
library(igraph)

# Split bigram column into word1 and word2
bigram_split <- executive_orders_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# Filter out rows with stop words for either word of the bigram
bigram_filtered <- bigram_split %>%
  anti_join(stop_words, by = c("word1" = "word")) %>%
  anti_join(stop_words, by = c("word2" = "word"))

# Count the number of appearances of each bigram
bigram_counts <- bigram_filtered %>%
  count(word1, word2, sort = TRUE)

# Filter to rows with more than 150 appearances
bigram_150 <- bigram_counts %>%
  filter(n > 150)

# Visualize the data
bigram_graph <- bigram_150 %>%
  graph_from_data_frame()

# Plot the relationships
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```

## Part c
```{r}

# Merge individual word columns back into a single bigram column
bigram_filtered <- bigram_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# Count the number of appearances of each bigram-president pair
bigram_president_counts <- bigram_filtered %>%
  count(president, bigram)

# Calculate TF-IDF
tfidf <- bigram_president_counts %>%
  bind_tf_idf(bigram, president, n)

# View the resulting TF-IDF dataframe
head(tfidf)

```

## Part d
```{r}
library(ggplot2)


# Get the top 15 bigrams with the largest TF-IDF values for each president
top_bigrams <- tfidf %>%
  group_by(president) %>%
  top_n(15, tf_idf) %>%
  arrange(president, desc(tf_idf))

# Plot the top bigrams for each president
ggplot(top_bigrams, aes(x = reorder(bigram, tf_idf), y = tf_idf, fill = president)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ president, scales = "free") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Bigram", y = "TF-IDF Value", title = "Top 15 Bigrams with Largest TF-IDF Values for Each President")

```
# Exercise 04
## Part a

```{r}
bills <- read_csv("senate_bills_114.csv") %>%
     mutate(passed = factor(passed, labels = c("1", "0"), levels = c("1", "0")))

# Count how many bills in the dataset passed
sum(bills$passed == "1")

```

108 bills in the dataset passed

## Part b
```{r}
# Set seed for reproducibility
set.seed(20220414)

# Drop the bill_number column
bills <- select(bills, -bill_number)

# Split the dataset into training and testing sets
bills_split <- initial_split(bills, prop = 0.75, strata = passed)
bills_train <- training(bills_split)
bills_test <- testing(bills_split)

```

## Part c
```{r}
# Load required libraries
install.packages("stopwords")
install.packages("textrecipes")

library(textrecipes)
library(stopwords)
library(tidytext)
library(tidymodels)
library(vip)

# Create a custom list of stopwords
custom_stopwords <- stopwords("en")

# Create the text processing recipe
text_recipe <- recipe(passed ~ description, data = bills_train) %>%
  step_tokenize(description) %>%
  step_stopwords(description, custom_stopwords = custom_stopwords) %>%
  step_stem(description) %>%
  step_tokenfilter(description, max_tokens = 200) %>%
  step_tfidf(description)
              
```

## Part d
```{r}

```

