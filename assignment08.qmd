---
title: "Data Science for Public Policy"
subtitle: "Assignment 08"
author: "Priscila Stisman / Asad Azhar"
execute:
  warning: false
format:
  html:
    embed-resources: true
---

### Packages

```{r}
library (tidyverse)
library(tidymodels)
library(lubridate)
library(themis)
library(yardstick)
library(vip)
library(kknn)
library(patchwork)
library(tidyclust)
library(purrr)
library(readr)
library(tidytext)
library(ggraph)
library(igraph)
library(ggplot2)
library(textrecipes)
library(stopwords)

```

# Exercise 1

## Part a

```{r}
votes_103 <- read_csv("votes_time_series.csv") %>%
  mutate_all(~ifelse(is.na(.), 0, .)) %>%
  filter(session == 103)

```

## Part b

```{r}

#Creating the recipe
votes_pca_rec <- votes_103 %>%
  recipe() %>%
  step_pca(starts_with("v"), num_comp = 5) %>%
  prep()


```

## Part c

```{r}

#Analysing PCs
tidy(votes_pca_rec, number = 1, type = "variance") %>%
  filter(terms == "cumulative percent variance") %>%
  slice_min(value, n = 5)

#The first PC explains around 39% of the variance while all of 5 of them cumulatively explain 68% of the variance.

## Part d


# Analysing PCs
votes_pcs <- votes_pca_rec %>%
  bake(new_data = votes_103)

# Loading and cleaning states data
states <- read_csv("states_regions.csv") %>%
  janitor::clean_names() %>%
  rename(state_name = state, state= state_code)

# Appending states data with votes PC data
votes_pcs <- left_join(votes_pcs, states, by = "state")

# Scatterplot with color mapped to party
plot_party <- ggplot(data = votes_pcs, aes(x = PC1, y = PC2, color = party)) +
  geom_point() +
  labs(title = "Scatterplot of PC1 and PC2 (Color by Party)",
       x = "PC1", y = "PC2")

# Scatterplot with color mapped to region
plot_region <- ggplot(data = votes_pcs, aes(x = PC1, y = PC2, color = region)) +
  geom_point() +
  labs(title = "Scatterplot of PC1 and PC2 (Color by Region)",
       x = "PC1", y = "PC2")

# Arrange plots side by side
combined_plots <- plot_party + plot_region

combined_plots

```

# Exercise 2

## Part a

```{r}
# Set seed for reproducibility
set.seed(20220412)

kmeans_rec <- recipe(formula = ~ .,data = votes_103) %>%
  step_select(all_numeric())

votes_cv <- vfold_cv(votes_103, v = 5)

kmeans_spec <- k_means(
  num_clusters = tune()) %>%
set_engine("stats", nstart = 100)

kmeans_wflow <- workflow(
  preprocessor = kmeans_rec,
  spec = kmeans_spec)

clust_num_grid <- grid_regular(num_clusters(),levels = 10)

res <- tune_cluster(kmeans_wflow,
                    resamples = votes_cv,
                    grid = clust_num_grid,
                    control = control_grid(save_pred = TRUE, extract = identity),
                    metrics = cluster_metric_set(sse_within_total, silhouette_avg))


res_metrics <- res %>%
  collect_metrics()

```

## Part b

```{r}

graph_pca_cluster <- function(df, clust) {
  
  # Creating the recipe for PCA
  rec_pca <- recipe(formula = ~ .,data = df) %>%
  step_pca(all_numeric(), id = "pca")
  
  # Creating recipe for kmeans
  kmeans_rec <- recipe(formula = ~ .,data = df) %>%
  step_select(all_numeric())
  
  #Completing PCA
  votes_pca <- rec_pca %>%
    prep() %>%
    bake(new_data = df)
  
  # Creating spec for clusters
  kmeans_spec <- k_means(num_clusters = clust) %>%
    set_engine("stats",nstart = 100)
  
  #Creating workflow
  kmeans_wflow <- workflow(
    preprocessor = kmeans_rec,
    spec = kmeans_spec)
  
  #Fitting data
  kmeans<- kmeans_wflow %>%
    fit(data = df)
  
  #Graphing PCA1 and PCA2 with color coded by cluster assigned.
  graph <- bind_cols(
    select(votes_pca, PC1, PC2),
    cluster = kmeans %>%
      extract_cluster_assignment() %>%
      pull(.cluster)
)
  ggplot(graph, aes(x = PC1, y = PC2, color = factor(cluster))) +
    geom_point() +
    labs(title = paste("K-Means with K =", clust, "and PCA"),
         x = "PC1", y = "PC2", color = "Cluster") +
    theme_minimal()
}

# Testing the function
graph_pca_cluster(df = votes_103, clust = 5)




```

# Exercise 03

## Part a

```{r}
#Read Executive Orders
executive_orders <- read_csv("executive-orders.csv") %>%
  filter(!is.na(text)) # Filter out rows with missing text

# Create bigrams using unnest_tokens
executive_orders_bigrams <- executive_orders %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

## Part b

```{r}
# Split bigram column into word1 and word2
bigram_split <- executive_orders_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# Filter out rows with stop words for either word of the bigram
bigram_filtered <- bigram_split %>%
  anti_join(stop_words, by = c("word1" = "word")) %>%
  anti_join(stop_words, by = c("word2" = "word"))

# Count the number of appearances of each bigram
bigram_counts <- bigram_filtered %>%
  count(word1, word2, sort = TRUE)

# Filter to rows with more than 150 appearances
bigram_150 <- bigram_counts %>%
  filter(n > 150)

# Visualize the data
bigram_graph <- bigram_150 %>%
  graph_from_data_frame()

# Plot the relationships
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```

## Part c

```{r}

# Merge individual word columns back into a single bigram column
bigram_filtered <- bigram_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# Count the number of appearances of each bigram-president pair
bigram_president_counts <- bigram_filtered %>%
  count(president, bigram)

# Calculate TF-IDF: TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.

tfidf <- bigram_president_counts %>%
  bind_tf_idf(bigram, president, n) 

# View the resulting TF-IDF dataframe
head(tfidf)

```

## Part d

```{r}


# Get the top 15 bigrams with the largest TF-IDF values for each president
top_bigrams <- tfidf %>%
  group_by(president) %>%
  top_n(15, tf_idf) %>%
  arrange(president, desc(tf_idf))

# Plot the top bigrams for each president
ggplot(top_bigrams, aes(x = reorder(bigram, tf_idf), y = tf_idf, fill = president)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ president, scales = "free") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Bigram", y = "TF-IDF Value", title = "Top 15 Bigrams with Largest TF-IDF Values for Each President")

```

# Exercise 04

## Part a

```{r}
bills <- read_csv("senate_bills_114.csv") %>%
     mutate(passed = factor(passed, labels = c("1", "0"), levels = c("1", "0")))

# Count how many bills in the dataset passed
sum(bills$passed == "1")

```

108 bills in the dataset passed

## Part b

```{r}
# Set seed for reproducibility
set.seed(20220414)

# Drop the bill_number column
bills <- select(bills, -bill_number)

# Split the dataset into training and testing sets
bills_split <- initial_split(bills, prop = 0.75, strata = passed)
bills_train <- training(bills_split)
bills_test <- testing(bills_split)

```

## Part c

```{r}

# Create a custom list of stopwords
custom_stopwords <- stopwords("en")

# Create the text processing recipe
bills_recipe <- recipe(passed ~ description, data = bills_train) %>%
  step_tokenize(description) %>%
  step_stopwords(description, custom_stopwords = all_of(custom_stopwords)) %>%
  step_stem(description) %>%
  step_tokenfilter(description, max_tokens = 200) %>%
  step_tfidf(description)
              
```

## Part d

```{r}

# Re-creating the recipe with domain-specific stop words
custom_stopwords <- c("1", "2", "3", "4", "act", "addit", "bill", "certain", "congress")

bills_recipe_2 <- recipe(passed ~ description, data = bills_train) %>%
  step_tokenize(description) %>%
  step_stopwords(description, custom_stopword_source = custom_stopwords) %>%
  step_stem(description) %>%
  step_tokenfilter(description, max_tokens = 200) %>%
  step_tfidf(description)
```

## Part e

```{r}
# Define the logistic regression model
logistic_reg <- logistic_reg() %>%
  set_mode("classification") %>%
  set_engine("glm") 

# Create a workflow with the recipe and logistic specification
bills_workflow <- workflow() %>%
  add_recipe(bills_recipe_2) %>%
  add_model(logistic_reg)

# Fit the model on the training data
fitted_model <- fit(bills_workflow, data = bills_train)
```

```{r}
# Predict the class membership and class probability using the test data
bills_predictions <- bind_cols(bills_test, 
                      predict(object = fitted_model, new_data = bills_test, type = "prob"),
                      predict(object = fitted_model, new_data = bills_test))

# Calculate accuracy, precision, recall and the roc curve for the model
accuracy <- accuracy(bills_predictions, truth = passed, estimate = .pred_class)
precision <- precision(bills_predictions, truth = passed, estimate = .pred_class)
recall <- recall(bills_predictions, truth = passed, estimate = .pred_class)
roccurve <- roc_curve(data = bills_predictions, .pred_1, truth = passed)

#Print the metrics and plot roc curve
metrics <- bind_rows(accuracy, precision, recall)
print(metrics)
autoplot(roccurve)

```

-   **Accuracy:** This metric indicates the overall correctness of the model's predictions. An accuracy of 96.62% suggests that the model correctly predicted the outcome for approximately 96.62% of the samples. However, it's important to note that the dataset is unbalanced, with positives representing only about 3% of all samples. As a result, the high accuracy figure may not accurately reflect the model's performance given the class imbalance.

-   **Precision:** Precision measures the proportion of true positive predictions among all positive predictions made by the model. A precision of 14.29% suggests that among the instances predicted as positive, only 14.29% were actually positive. This low precision value indicates that our model is performing poorly in accurately identifying positive instances.

-   **Recall:** Recall, also known as sensitivity, measures the proportion of actual positive instances that were correctly identified by the model. A recall of 10.00% suggests that the model correctly identified only 10.00% of all positive instances. This low recall value indicates that our model is performing poorly in capturing the true positive instances.

-   **ROC:** The area under the ROC curve summarizes the overall performance of the classifier across all possible threshold values. A higher AUC value (closer to 1) indicates better discrimination ability of the model. The area under the ROC curve in our model is closer to 0.5 suggesting a random classifier.

*Recommendations to improve the model*

1.   Address Class Imabalance: implement resampling methods to balance classes, for example, oversampling the minority class.

2.  Feature Engineering: Explore additional features or transformations of existing features that could better capture underlying patterns in the data. For example, adding interaction terms, or scaling data.

3.  Increase sample size: Expanding the sample size is a well-known strategy that can significantly enhance the accuracy and reliability of model estimations. However, we acknowledge that increasing the sample size may present logistical challenges or resource constraints, making it a less practical solution in some cases.
